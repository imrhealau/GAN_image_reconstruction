{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6qRWQJzj9TA",
        "outputId": "2f1a7726-6830-46bb-e3b5-3246af8e6093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "# !pip install torchinfo\n",
        "import torchinfo\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ShdKBIMbj9TB"
      },
      "outputs": [],
      "source": [
        "#=======================Define masking function=======================#\n",
        "class Mask(object):\n",
        "    def __init__(self, masking_rate):\n",
        "        self.masking_rate = masking_rate\n",
        "\n",
        "    def __call__(self, matrix_3d):\n",
        "        num_columns_to_mask = int(self.masking_rate * matrix_3d.shape[2])\n",
        "        columns_to_mask = np.random.choice(matrix_3d.shape[2], num_columns_to_mask, replace=False)\n",
        "        matrix_3d_mask = matrix_3d.clone()\n",
        "        matrix_3d_mask[:,:, columns_to_mask] = 0\n",
        "        return matrix_3d, matrix_3d_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "S43-IAuVj9TB"
      },
      "outputs": [],
      "source": [
        "#=======================Training parameters=======================#\n",
        "masking_rate = 0.5   # masking rate\n",
        "bs = 64              # batch size (og:64)\n",
        "lr = 0.0002          # learning rate (og:0.0002)\n",
        "n_epoch = 10         # number of epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iEYkC-_lj9TB"
      },
      "outputs": [],
      "source": [
        "# #=======================Create F-MNIST datasets=======================#\n",
        "# ## create two transform (G has masked data and D has original data)\n",
        "# mask = Mask(masking_rate)\n",
        "# transform_d_g = transforms.Compose([transforms.ToTensor(),mask])\n",
        "\n",
        "# train_dataset_d_g = datasets.FashionMNIST(root='./F_mnist_data/', train=True, transform=transform_d_g, download=True)\n",
        "# train_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=True)\n",
        "\n",
        "# validation_dataset_d_g = datasets.FashionMNIST(root='./F_mnist_data/', train=False, transform=transform_d_g, download=False)\n",
        "# validation_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=False)\n",
        "\n",
        "# test_dataset_d_g = datasets.FashionMNIST(root='./F_mnist_data/', train=False, transform=transform_d_g, download=False)\n",
        "# test_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WOofTEX54aUW"
      },
      "outputs": [],
      "source": [
        "#=======================Create MNIST datasets=======================#\n",
        "## create two transform (G has masked data and D has original data)\n",
        "mask = Mask(masking_rate)\n",
        "transform_d_g = transforms.Compose([transforms.ToTensor(),mask])\n",
        "\n",
        "train_dataset_d_g = datasets.MNIST(root='./mnist_data/', train=True, transform=transform_d_g, download=True)\n",
        "train_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=True)\n",
        "\n",
        "validation_dataset_d_g = datasets.MNIST(root='./mnist_data/', train=False, transform=transform_d_g, download=False)\n",
        "validation_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=False)\n",
        "\n",
        "test_dataset_d_g = datasets.MNIST(root='./mnist_data/', train=False, transform=transform_d_g, download=False)\n",
        "test_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7XwA6ey1j9TC"
      },
      "outputs": [],
      "source": [
        "#=======================Build GAN=======================#\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(g_input_dim, 512) # orginal 256\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, self.fc3.out_features*2)\n",
        "        self.fc5 = nn.Linear(self.fc4.out_features, g_output_dim)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        return torch.tanh(self.fc5(x))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.dropout(x, 0.3) # force the network use different neurons\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "\n",
        "        return x  #-1<x<1\n",
        "\n",
        "class LENET_Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(LENET_Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) #AutoML\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64*7*7,1024)  # Adjust the input size based on your input dimensions\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "        self.dropout = nn.Dropout(0.3)  # Adjust the dropout rate as needed\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DhjnEUO0j9TC"
      },
      "outputs": [],
      "source": [
        "#=======================Build the network=======================#\n",
        "z_dim = 28*28\n",
        "mnist_dim = train_dataset_d_g.data.size(1) * train_dataset_d_g.data.size(2)\n",
        "\n",
        "G = Generator(g_input_dim = mnist_dim, g_output_dim = mnist_dim).to(device)\n",
        "D = LENET_Discriminator(mnist_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrwfjDSdxvsA",
        "outputId": "ed1318e9-c888-4ea2-b79f-59cbf1eb6519"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "Generator                                --\n",
              "├─Linear: 1-1                            401,920\n",
              "├─Linear: 1-2                            525,312\n",
              "├─Linear: 1-3                            2,099,200\n",
              "├─Linear: 1-4                            8,392,704\n",
              "├─Linear: 1-5                            3,212,048\n",
              "=================================================================\n",
              "Total params: 14,631,184\n",
              "Trainable params: 14,631,184\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torchinfo.summary(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_sQuX-DSj9TC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/woodstock/Library/Python/3.9/lib/python/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ],
      "source": [
        "#=======================Choose loss and optimizer=======================#\n",
        "# loss\n",
        "criterion = nn.BCELoss() # binary crossentropy loss, normalised ./. two classes\n",
        "mse_loss = nn.MSELoss(reduce=True,reduction='sum')\n",
        "\n",
        "# optimizer\n",
        "G_optimizer = optim.Adam(G.parameters(), lr = lr)\n",
        "D_optimizer = optim.Adam(D.parameters(), lr = lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_lNQ7GIFj9TC"
      },
      "outputs": [],
      "source": [
        "def D_train(x_d,x_g):\n",
        "    #=======================Train the discriminator=======================#\n",
        "    D.zero_grad()\n",
        "\n",
        "    # train discriminator on real\n",
        "    bs = x_d.shape[0]\n",
        "    x_real, y_real = x_d.view(-1, mnist_dim), torch.ones(bs, 1)  # assigning the input image with 1 (real)\n",
        "    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n",
        "\n",
        "    D_output = D(x_d)\n",
        "    D_real_loss = criterion(D_output, y_real)\n",
        "\n",
        "    # train discriminator on fake\n",
        "    z = x_g.view(-1, mnist_dim) ##!!\n",
        "    x_fake, y_fake = G(z), Variable(torch.zeros(bs, 1).to(device)) # assigning the input image with 0 (fake)\n",
        "\n",
        "    x_fake = x_fake.view(x_fake.size(0), 1, 28, 28)\n",
        "    # D_output = D(x_fake)\n",
        "    D_fake_loss = criterion(D_output, y_fake)  # probability compare with 0\n",
        "\n",
        "    # gradient backprop & optimize ONLY D's parameters\n",
        "    D_loss = (D_real_loss + D_fake_loss)/2\n",
        "    D_loss.backward()\n",
        "    D_optimizer.step()\n",
        "\n",
        "    return  D_loss.data.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def D_val(x_d,x_g):\n",
        "    #=======================Train the discriminator=======================#\n",
        "    D.zero_grad()\n",
        "\n",
        "    # train discriminator on real\n",
        "    bs = x_d.shape[0]\n",
        "    x_real, y_real = x_d.view(-1, mnist_dim), torch.ones(bs, 1)  # assigning the input image with 1 (real)\n",
        "    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n",
        "\n",
        "    D_output = D(x_d)\n",
        "    D_real_loss = criterion(D_output, y_real)\n",
        "\n",
        "    # train discriminator on fake\n",
        "    z = x_g.view(-1, mnist_dim) ##!!\n",
        "    x_fake, y_fake = G(z), Variable(torch.zeros(bs, 1).to(device)) # assigning the input image with 0 (fake)\n",
        "\n",
        "    x_fake = x_fake.view(x_fake.size(0), 1, 28, 28)\n",
        "    # D_output = D(x_fake)\n",
        "    D_fake_loss = criterion(D_output, y_fake)  # probability compare with 0\n",
        "\n",
        "    # gradient backprop & optimize ONLY D's parameters\n",
        "    D_loss = (D_real_loss + D_fake_loss)/2\n",
        "\n",
        "    return  D_loss.data.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a8_B1AKQj9TD"
      },
      "outputs": [],
      "source": [
        "def G_train(x_d, x_g):\n",
        "    #=======================Train the generator=======================#\n",
        "    # print(x_g.shape)\n",
        "    bs = x_g.shape[0]\n",
        "    G.zero_grad()\n",
        "\n",
        "    z = x_g.view(-1, mnist_dim)\n",
        "    y = Variable(torch.ones(bs, 1).to(device))\n",
        "\n",
        "    G_output = G(z)\n",
        "    ##\n",
        "    G_output2 = G_output.view(G_output.size(0), 1, 28, 28)\n",
        "    D_output = D(G_output2)\n",
        "\n",
        "    # Reshape the tensor\n",
        "    x_d = x_d.view((x_d.size(0), -1))  # Keep the first dimension (batch size), flatten the rest\n",
        "    G_loss = criterion(D_output, y) + mse_loss(G_output, x_d)\n",
        "    m_loss = mse_loss(G_output, x_d)\n",
        "\n",
        "    # gradient backprop & optimize ONLY G's parameters\n",
        "    G_loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "    return G_loss.data.item(), m_loss.data.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def G_val(x_d, x_g):\n",
        "    #=======================Train the generator=======================#\n",
        "    # print(x_g.shape)\n",
        "    bs = x_g.shape[0]\n",
        "    G.zero_grad()\n",
        "\n",
        "    z = x_g.view(-1, mnist_dim)\n",
        "    y = Variable(torch.ones(bs, 1).to(device))\n",
        "\n",
        "    G_output = G(z)\n",
        "    ##\n",
        "    G_output2 = G_output.view(G_output.size(0), 1, 28, 28)\n",
        "    D_output = D(G_output2)\n",
        "\n",
        "    # Reshape the tensor\n",
        "    x_d = x_d.view((x_d.size(0), -1))  # Keep the first dimension (batch size), flatten the rest\n",
        "    G_loss = criterion(D_output, y) + mse_loss(G_output, x_d)\n",
        "    m_loss = mse_loss(G_output, x_d)\n",
        "\n",
        "    return G_loss.data.item(), m_loss.data.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk14QR-gj9TD",
        "outputId": "7caae4b7-5902-4137-e8b4-6e47e5d690c3"
      },
      "outputs": [],
      "source": [
        "# #===================Train the model===================#\n",
        "# for epoch in range(1, n_epoch+1):\n",
        "#     # print(next(iter(train_loader)))\n",
        "#     D_losses,G_losses, mse_losses = [],[],[]\n",
        "\n",
        "#     for batch_idx,(((x_d, x_g), _)) in enumerate(tqdm(train_loader_d_g)):\n",
        "#         x_d=x_d.to(device)\n",
        "#         x_g=x_g.to(device)\n",
        "#         G_losses.append(G_train(x_d, x_g)[0])\n",
        "#         D_losses.append(D_train(x_d, x_g))\n",
        "#         mse_losses.append(G_train(x_d, x_g)[1])\n",
        "\n",
        "#         # plot data\n",
        "#         if batch_idx == len(train_loader_d_g)-1:\n",
        "#             data = x_g[0][0]\n",
        "#             original = x_d[0][0]\n",
        "#             z = data.view(-1, mnist_dim)\n",
        "#             fig, ax = plt.subplots(1,3)\n",
        "#             ax[0].imshow(transforms.ToPILImage()(data), cmap='Greys')\n",
        "#             ax[1].imshow(G(z).cpu().detach().numpy().reshape(28,28), cmap='Greys')\n",
        "#             ax[2].imshow(transforms.ToPILImage()(original), cmap='Greys')\n",
        "#             plt.show()\n",
        "#         # break\n",
        "\n",
        "\n",
        "\n",
        "#     print('[%d/%d]: loss_d: %.5f, loss_g: %.5f, loss_mse: %.5f' % (\n",
        "#             (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses)), torch.mean(torch.FloatTensor(mse_losses))))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training [1/10]: loss_d: 0.69315, loss_g: 0.72573, loss_mse: 0.03215\n",
            "Validation [1/10]: loss_d: 0.69315, loss_g: 0.71732, loss_mse: 0.02404\n",
            "Training [2/10]: loss_d: 0.69315, loss_g: 0.71520, loss_mse: 0.02203\n",
            "Validation [2/10]: loss_d: 0.69315, loss_g: 0.71373, loss_mse: 0.02059\n",
            "Training [3/10]: loss_d: 0.69315, loss_g: 0.71276, loss_mse: 0.01957\n",
            "Validation [3/10]: loss_d: 0.69315, loss_g: 0.71198, loss_mse: 0.01880\n",
            "Training [4/10]: loss_d: 0.69315, loss_g: 0.71144, loss_mse: 0.01826\n",
            "Validation [4/10]: loss_d: 0.69315, loss_g: 0.71085, loss_mse: 0.01770\n",
            "Training [5/10]: loss_d: 0.69315, loss_g: 0.71051, loss_mse: 0.01735\n",
            "Validation [5/10]: loss_d: 0.69315, loss_g: 0.71010, loss_mse: 0.01695\n",
            "Training [6/10]: loss_d: 0.69315, loss_g: 0.70987, loss_mse: 0.01673\n",
            "Validation [6/10]: loss_d: 0.69315, loss_g: 0.70958, loss_mse: 0.01646\n",
            "Training [7/10]: loss_d: 0.69315, loss_g: 0.70935, loss_mse: 0.01620\n",
            "Validation [7/10]: loss_d: 0.69315, loss_g: 0.70913, loss_mse: 0.01599\n",
            "Training [8/10]: loss_d: 0.69315, loss_g: 0.70898, loss_mse: 0.01583\n",
            "Validation [8/10]: loss_d: 0.69315, loss_g: 0.70869, loss_mse: 0.01556\n",
            "Training [9/10]: loss_d: 0.69315, loss_g: 0.70865, loss_mse: 0.01550\n",
            "Validation [9/10]: loss_d: 0.69315, loss_g: 0.70843, loss_mse: 0.01526\n",
            "Training [10/10]: loss_d: 0.69315, loss_g: 0.70837, loss_mse: 0.01523\n",
            "Validation [10/10]: loss_d: 0.69315, loss_g: 0.70816, loss_mse: 0.01503\n"
          ]
        }
      ],
      "source": [
        "#===================Train the model===================#\n",
        "D_loss_train = []\n",
        "D_loss_val = []\n",
        "G_loss_train = []\n",
        "G_loss_val = []\n",
        "m_loss_train = []\n",
        "m_loss_val = []\n",
        "\n",
        "for epoch in range(1, n_epoch+1):     \n",
        "    # print(next(iter(train_loader)))   \n",
        "    D_losses,G_losses, mse_losses = [],[],[]\n",
        "\n",
        "    for batch_idx,(((x_d, x_g), _)) in enumerate(train_loader_d_g):\n",
        "        x_d=x_d.to(device)\n",
        "        x_g=x_g.to(device)  \n",
        "        gen_loss_first, mse_loss_first = G_train(x_d, x_g)\n",
        "        D_losses.append(D_train(x_d, x_g))\n",
        "        gen_loss_second, mse_loss_second = G_train(x_d, x_g)\n",
        "        G_losses.append((gen_loss_second + gen_loss_first)/2)\n",
        "        mse_losses.append((mse_loss_second + mse_loss_first)/2)\n",
        "        \n",
        "        # plot data\n",
        "        # if batch_idx == len(train_loader_d_g)-1:\n",
        "        #     data = x_g[0][0]\n",
        "        #     original = x_d[0][0]\n",
        "        #     z = data.view(-1, mnist_dim)\n",
        "        #     fig, ax = plt.subplots(1,3)\n",
        "        #     ax[0].imshow(transforms.ToPILImage()(data), cmap='Greys')\n",
        "        #     ax[1].imshow(G(z).cpu().detach().numpy().reshape(28,28), cmap='Greys')\n",
        "        #     ax[2].imshow(transforms.ToPILImage()(original), cmap='Greys')\n",
        "        #     plt.show()\n",
        "        # break\n",
        "\n",
        "    D_loss_train.append(torch.mean(torch.FloatTensor(D_losses)).item())\n",
        "    G_loss_train.append(torch.mean(torch.FloatTensor(G_losses)).item())\n",
        "    m_loss_train.append(torch.mean(torch.FloatTensor(mse_losses)).item())\n",
        "\n",
        "    print('Training [%d/%d]: loss_d: %.5f, loss_g: %.5f, loss_mse: %.5f' % (\n",
        "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses)), torch.mean(torch.FloatTensor(mse_losses))))\n",
        "    \n",
        "    # validation\n",
        "    D_losses,G_losses, mse_losses = [],[],[]\n",
        "    for batch_idx,(((x_d, x_g), _)) in enumerate(test_loader_d_g):\n",
        "        x_d=x_d.to(device)\n",
        "        x_g=x_g.to(device)\n",
        "        G_losses.append(G_val(x_d, x_g)[0])\n",
        "        D_losses.append(D_val(x_d, x_g))\n",
        "        mse_losses.append(G_val(x_d, x_g)[1])\n",
        "    \n",
        "    D_loss_val.append(torch.mean(torch.FloatTensor(D_losses)).item())\n",
        "    G_loss_val.append(torch.mean(torch.FloatTensor(G_losses)).item())\n",
        "    m_loss_val.append(torch.mean(torch.FloatTensor(mse_losses)).item())\n",
        "        \n",
        "    print('Validation [%d/%d]: loss_d: %.5f, loss_g: %.5f, loss_mse: %.5f' % (\n",
        "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses)), torch.mean(torch.FloatTensor(mse_losses))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'D_loss_train': [0.6931501626968384, 0.6931477785110474, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339], 'D_loss_val': [0.6931480765342712, 0.693147599697113, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339, 0.6931473612785339], 'G_loss_train': [0.7257280349731445, 0.7152027487754822, 0.712761640548706, 0.7114360332489014, 0.7105093598365784, 0.7098749876022339, 0.7093514204025269, 0.7089822292327881, 0.708646833896637, 0.7083731293678284], 'G_loss_val': [0.7173208594322205, 0.7137323617935181, 0.7119789123535156, 0.7108508944511414, 0.7101026177406311, 0.7095806002616882, 0.709132194519043, 0.7086949348449707, 0.7084313035011292, 0.7081576585769653], 'm_loss_train': [0.03214597702026367, 0.022028082981705666, 0.019572608172893524, 0.01826498843729496, 0.017354104667901993, 0.016725346446037292, 0.016203144565224648, 0.015834566205739975, 0.015499448403716087, 0.015225694514811039], 'm_loss_val': [0.02404477819800377, 0.02058941125869751, 0.01879982464015484, 0.017695600166916847, 0.016945790499448776, 0.016456937417387962, 0.015989718958735466, 0.015562055632472038, 0.015261775813996792, 0.015028676018118858]}\n"
          ]
        }
      ],
      "source": [
        "loss_dict = {'D_loss_train': D_loss_train,\n",
        "'D_loss_val': D_loss_val,\n",
        "'G_loss_train': G_loss_train,\n",
        "'G_loss_val': G_loss_val,\n",
        "'m_loss_train': m_loss_train,\n",
        "'m_loss_val': m_loss_val}\n",
        "\n",
        "print(loss_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "p41Pd_rEj9TD"
      },
      "outputs": [],
      "source": [
        "# #===================Test the model===================#\n",
        "# index = 6\n",
        "\n",
        "# for batch_idx,(((x_d, x_g), _)) in enumerate(test_loader_d_g):\n",
        "#     x_d=x_d.to(device)\n",
        "#     x_g=x_g.to(device)\n",
        "#     if batch_idx == index:\n",
        "#         data = x_g[0][0]\n",
        "#         original = x_d[0][0]\n",
        "#         z = data.view(-1, mnist_dim)\n",
        "\n",
        "#         fig, ax = plt.subplots(1,3)\n",
        "#         ax[0].imshow(transforms.ToPILImage()(data), cmap='Greys')\n",
        "#         ax[1].imshow(G(z).cpu().detach().numpy().reshape(28,28), cmap='Greys')\n",
        "#         ax[2].imshow(transforms.ToPILImage()(original), cmap='Greys')\n",
        "#         plt.show()\n",
        "\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4oJFWYtfx7Me"
      },
      "outputs": [],
      "source": [
        "# # Save the generator model\n",
        "# torch.save(G, 'trained_model/G_mnist_lenet.pth')\n",
        "# g_state_dict = G.state_dict()\n",
        "# torch.save(g_state_dict, 'trained_model/G_mnist_lenet_weights.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#===================Test the model===================#\n",
        "data=torch.load('sample_pics/mask_six.pth')\n",
        "original=torch.load('sample_pics/og_six.pth')\n",
        "\n",
        "z = data.view(-1, mnist_dim).to(device)\n",
        "\n",
        "fig, ax = plt.subplots(1,3)\n",
        "ax[0].imshow(transforms.ToPILImage()(data), cmap='Greys')\n",
        "ax[0].set_title(f'Image with {int(masking_rate*100)}% missing traces',size=8)\n",
        "ax[1].imshow(G(z).cpu().detach().numpy().reshape(28,28), cmap='Greys')\n",
        "ax[1].set_title(f'Linear model result',size=8)\n",
        "ax[2].imshow(transforms.ToPILImage()(original), cmap='Greys')\n",
        "ax[2].set_title(f'Original image',size=8)\n",
        "# plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
