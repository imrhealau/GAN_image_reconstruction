{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# !pip install torchinfo\n",
    "import torchinfo\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================Define masking function=======================#\n",
    "class Mask(object):\n",
    "    def __init__(self, masking_rate):\n",
    "        self.masking_rate = masking_rate\n",
    "\n",
    "    def __call__(self, matrix_3d):\n",
    "        num_columns_to_mask = int(self.masking_rate * matrix_3d.shape[2])\n",
    "        columns_to_mask = np.random.choice(matrix_3d.shape[2], num_columns_to_mask, replace=False)\n",
    "        matrix_3d_mask = matrix_3d.clone()\n",
    "        matrix_3d_mask[:,:, columns_to_mask] = 0\n",
    "        return matrix_3d, matrix_3d_mask\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================Training parameters=======================#\n",
    "bs = 64             # batch size\n",
    "masking_rate = 0.5  # masking rate\n",
    "lr = 0.0002         # learning rate\n",
    "n_epoch = 10        # number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================Create MNIST datasets=======================#\n",
    "## create two transform (G has masked data and D has original data)\n",
    "mask = Mask(masking_rate)\n",
    "transform_d_g = transforms.Compose([transforms.ToTensor(),mask])\n",
    "\n",
    "train_dataset_d_g = datasets.MNIST(root='./mnist_data/', train=True, transform=transform_d_g, download=True)\n",
    "train_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=True)\n",
    "\n",
    "validation_dataset_d_g = datasets.MNIST(root='./mnist_data/', train=False, transform=transform_d_g, download=False)\n",
    "validation_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=False)\n",
    "\n",
    "test_dataset_d_g = datasets.MNIST(root='./mnist_data/', train=False, transform=transform_d_g, download=False)\n",
    "test_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #=======================Create F-MNIST datasets=======================#\n",
    "# ## create two transform (G has masked data and D has original data)\n",
    "# mask = Mask(masking_rate)\n",
    "# transform_d_g = transforms.Compose([transforms.ToTensor(),mask])\n",
    "\n",
    "# train_dataset_d_g = datasets.FashionMNIST(root='./F_mnist_data/', train=True, transform=transform_d_g, download=True)\n",
    "# train_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=True)\n",
    "\n",
    "# validation_dataset_d_g = datasets.FashionMNIST(root='./F_mnist_data/', train=False, transform=transform_d_g, download=False)\n",
    "# validation_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=False)\n",
    "\n",
    "# test_dataset_d_g = datasets.FashionMNIST(root='./F_mnist_data/', train=False, transform=transform_d_g, download=False)\n",
    "# test_loader_d_g = torch.utils.data.DataLoader(dataset=train_dataset_d_g, batch_size=bs, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================Build GAN=======================#\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, g_input_dim, g_output_dim):\n",
    "        super(Generator, self).__init__()       \n",
    "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
    "    \n",
    "    # forward method\n",
    "    def forward(self, x): \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return torch.tanh(self.fc4(x))\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d_input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
    "    \n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, 0.3) # force the network use different neurons\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "\n",
    "        return x  #-1<x<1\n",
    "\n",
    "class CNN_Discriminator(nn.Module):\n",
    "    def __init__(self, d_input_dim):\n",
    "        super(CNN_Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) #AutoML\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64*7*7,1024)  # Adjust the input size based on your input dimensions\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
    "        self.dropout = nn.Dropout(0.3)  # Adjust the dropout rate as needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================Build the network=======================#\n",
    "z_dim = 28*28\n",
    "mnist_dim = train_dataset_d_g.data.size(1) * train_dataset_d_g.data.size(2)\n",
    "\n",
    "G = Generator(g_input_dim = mnist_dim, g_output_dim = mnist_dim).to(device)\n",
    "D = Discriminator(mnist_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================Choose loss and optimizer=======================#\n",
    "# loss\n",
    "criterion = nn.BCELoss() # binary crossentropy loss, normalised ./. two classes\n",
    "mse_loss = nn.MSELoss(reduce=True,reduction='sum')\n",
    "\n",
    "# optimizer\n",
    "G_optimizer = optim.Adam(G.parameters(), lr = lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_train(x_d,x_g):\n",
    "    #=======================Train the discriminator=======================#\n",
    "    D.zero_grad()\n",
    "\n",
    "    # train discriminator on real\n",
    "    bs = x_d.shape[0]\n",
    "    # x_d = torch.mean(x, dim = 1)\n",
    "    x_real, y_real = x_d.view(-1, mnist_dim), torch.ones(bs, 1)  # assigning the input image with 1 (real)\n",
    "    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n",
    "\n",
    "    # D_output = D(x_d)\n",
    "    D_output = D(x_real)\n",
    "    # print(D_output.shape, y_real.shape, x_real.shape)\n",
    "    D_real_loss = criterion(D_output, y_real)\n",
    "\n",
    "    # train discriminator on fake\n",
    "    z = x_g.view(-1, mnist_dim) ##!!\n",
    "    x_fake, y_fake = G(z), Variable(torch.zeros(bs, 1).to(device)) # assigning the input image with 0 (fake)\n",
    "\n",
    "    # x_fake = x_fake.view(x_fake.size(0), 1, 28, 28)\n",
    "    D_output = D(x_fake)\n",
    "    D_fake_loss = criterion(D_output, y_fake)  # probability compare with 0\n",
    "\n",
    "    # gradient backprop & optimize ONLY D's parameters\n",
    "    D_loss = (D_real_loss + D_fake_loss)/2\n",
    "    D_loss.backward()\n",
    "    D_optimizer.step()\n",
    "        \n",
    "    return  D_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_val(x_d,x_g):\n",
    "    #=======================Train the discriminator=======================#\n",
    "    D.zero_grad()\n",
    "\n",
    "    # train discriminator on real\n",
    "    bs = x_d.shape[0]\n",
    "    # x_d = torch.mean(x, dim = 1)\n",
    "    x_real, y_real = x_d.view(-1, mnist_dim), torch.ones(bs, 1)  # assigning the input image with 1 (real)\n",
    "    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n",
    "\n",
    "    # D_output = D(x_d)\n",
    "    D_output = D(x_real)\n",
    "    # print(D_output.shape, y_real.shape, x_real.shape)\n",
    "    D_real_loss = criterion(D_output, y_real)\n",
    "\n",
    "    # train discriminator on fake\n",
    "    z = x_g.view(-1, mnist_dim) ##!!\n",
    "    x_fake, y_fake = G(z), Variable(torch.zeros(bs, 1).to(device)) # assigning the input image with 0 (fake)\n",
    "\n",
    "    # x_fake = x_fake.view(x_fake.size(0), 1, 28, 28)\n",
    "    D_output = D(x_fake)\n",
    "    D_fake_loss = criterion(D_output, y_fake)  # probability compare with 0\n",
    "\n",
    "    D_loss = (D_real_loss + D_fake_loss)/2\n",
    "        \n",
    "    return  D_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_train(x_d, x_g):\n",
    "    #=======================Train the generator=======================#\n",
    "    # print(x_g.shape)\n",
    "    bs = x_g.shape[0]\n",
    "    G.zero_grad()\n",
    "\n",
    "    z = x_g.view(-1, mnist_dim)\n",
    "    y = Variable(torch.ones(bs, 1).to(device))\n",
    "    \n",
    "    G_output = G(z)\n",
    "    ##\n",
    "    # G_output2 = G_output.view(G_output.size(0), 1, 28, 28)\n",
    "    D_output = D(G_output)\n",
    "\n",
    "    # Reshape the tensor\n",
    "    x_d = x_d.view((x_d.size(0), -1))  # Keep the first dimension (batch size), flatten the rest\n",
    "    G_loss = criterion(D_output, y) + mse_loss(G_output, x_d)\n",
    "    # bce_loss = criterion(D_output, y)\n",
    "    m_loss = mse_loss(G_output, x_d)\n",
    "    \n",
    "    # gradient backprop & optimize ONLY G's parameters\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "        \n",
    "    return G_loss.data.item(), m_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_val(x_d, x_g):\n",
    "    #=======================Train the generator=======================#\n",
    "    # print(x_g.shape)\n",
    "    bs = x_g.shape[0]\n",
    "    G.zero_grad()\n",
    "\n",
    "    z = x_g.view(-1, mnist_dim)\n",
    "    y = Variable(torch.ones(bs, 1).to(device))\n",
    "    \n",
    "    G_output = G(z)\n",
    "    ##\n",
    "    # G_output2 = G_output.view(G_output.size(0), 1, 28, 28)\n",
    "    D_output = D(G_output)\n",
    "\n",
    "    # Reshape the tensor\n",
    "    x_d = x_d.view((x_d.size(0), -1))  # Keep the first dimension (batch size), flatten the rest\n",
    "    G_loss = criterion(D_output, y) + mse_loss(G_output, x_d)\n",
    "    # bce_loss = criterion(D_output, y)\n",
    "    m_loss = mse_loss(G_output, x_d)\n",
    "\n",
    "    return G_loss.data.item(), m_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #===================Train the model===================#\n",
    "# for epoch in range(1, n_epoch+1):     \n",
    "#     # print(next(iter(train_loader)))   \n",
    "#     D_losses,G_losses, mse_losses = [],[],[]\n",
    "\n",
    "#     for batch_idx,(((x_d, x_g), _)) in enumerate(tqdm(train_loader_d_g)):\n",
    "#         G_losses.append(G_train(x_d, x_g)[0])\n",
    "#         D_losses.append(D_train(x_d, x_g))\n",
    "#         mse_losses.append(G_train(x_d, x_g)[1])\n",
    "\n",
    "#         # plot data\n",
    "#         if batch_idx == len(train_loader_d_g)-1:\n",
    "#             data = x_g[0][0]\n",
    "#             original = x_d[0][0]\n",
    "#             z = data.view(-1, mnist_dim)\n",
    "#             fig, ax = plt.subplots(1,3)\n",
    "#             ax[0].imshow(transforms.ToPILImage()(data), cmap='Greys')\n",
    "#             ax[1].imshow(G(z).detach().numpy().reshape(28,28), cmap='Greys')\n",
    "#             ax[2].imshow(transforms.ToPILImage()(original), cmap='Greys')\n",
    "#             plt.show()\n",
    "#         # break\n",
    "\n",
    "        \n",
    "#     print('[%d/%d]: loss_d: %.3f, loss_g: %.3f, loss_mse: %.3f' % (\n",
    "#             (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses)), torch.mean(torch.FloatTensor(mse_losses))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [1/10]: loss_d: 0.69596, loss_g: 0.89239, loss_mse: 0.10780\n",
      "Validation [1/10]: loss_d: 0.62340, loss_g: 0.98929, loss_mse: 0.11143\n",
      "Training [2/10]: loss_d: 0.69375, loss_g: 0.85391, loss_mse: 0.09724\n",
      "Validation [2/10]: loss_d: 0.68821, loss_g: 0.76498, loss_mse: 0.07881\n",
      "Training [3/10]: loss_d: 0.69338, loss_g: 0.74560, loss_mse: 0.05166\n",
      "Validation [3/10]: loss_d: 0.69280, loss_g: 0.73769, loss_mse: 0.04282\n",
      "Training [4/10]: loss_d: 0.69309, loss_g: 0.73168, loss_mse: 0.03851\n",
      "Validation [4/10]: loss_d: 0.69276, loss_g: 0.72734, loss_mse: 0.03394\n",
      "Training [5/10]: loss_d: 0.69306, loss_g: 0.72456, loss_mse: 0.03213\n",
      "Validation [5/10]: loss_d: 0.69291, loss_g: 0.72281, loss_mse: 0.02965\n",
      "Training [6/10]: loss_d: 0.69310, loss_g: 0.72101, loss_mse: 0.02843\n",
      "Validation [6/10]: loss_d: 0.69282, loss_g: 0.72263, loss_mse: 0.02635\n",
      "Training [7/10]: loss_d: 0.69313, loss_g: 0.71985, loss_mse: 0.02595\n",
      "Validation [7/10]: loss_d: 0.69291, loss_g: 0.72055, loss_mse: 0.02492\n",
      "Training [8/10]: loss_d: 0.69314, loss_g: 0.71783, loss_mse: 0.02430\n",
      "Validation [8/10]: loss_d: 0.69321, loss_g: 0.72180, loss_mse: 0.02491\n",
      "Training [9/10]: loss_d: 0.69313, loss_g: 0.71660, loss_mse: 0.02295\n",
      "Validation [9/10]: loss_d: 0.69267, loss_g: 0.71586, loss_mse: 0.02146\n",
      "Training [10/10]: loss_d: 0.69306, loss_g: 0.71613, loss_mse: 0.02246\n",
      "Validation [10/10]: loss_d: 0.69252, loss_g: 0.71657, loss_mse: 0.02133\n"
     ]
    }
   ],
   "source": [
    "#===================Train the model===================#\n",
    "D_loss_train = []\n",
    "D_loss_val = []\n",
    "G_loss_train = []\n",
    "G_loss_val = []\n",
    "m_loss_train = []\n",
    "m_loss_val = []\n",
    "\n",
    "for epoch in range(1, n_epoch+1):     \n",
    "    # print(next(iter(train_loader)))   \n",
    "    D_losses,G_losses, mse_losses = [],[],[]\n",
    "\n",
    "    for batch_idx,(((x_d, x_g), _)) in enumerate(train_loader_d_g):\n",
    "        x_d=x_d.to(device)\n",
    "        x_g=x_g.to(device)  \n",
    "        gen_loss_first, mse_loss_first = G_train(x_d, x_g)\n",
    "        D_losses.append(D_train(x_d, x_g))\n",
    "        gen_loss_second, mse_loss_second = G_train(x_d, x_g)\n",
    "        G_losses.append((gen_loss_second + gen_loss_first)/2)\n",
    "        mse_losses.append((mse_loss_second + mse_loss_first)/2)\n",
    "        \n",
    "        # plot data\n",
    "        # if batch_idx == len(train_loader_d_g)-1:\n",
    "        #     data = x_g[0][0]\n",
    "        #     original = x_d[0][0]\n",
    "        #     z = data.view(-1, mnist_dim)\n",
    "        #     fig, ax = plt.subplots(1,3)\n",
    "        #     ax[0].imshow(transforms.ToPILImage()(data), cmap='Greys')\n",
    "        #     ax[1].imshow(G(z).cpu().detach().numpy().reshape(28,28), cmap='Greys')\n",
    "        #     ax[2].imshow(transforms.ToPILImage()(original), cmap='Greys')\n",
    "        #     plt.show()\n",
    "        # break\n",
    "\n",
    "    D_loss_train.append(torch.mean(torch.FloatTensor(D_losses)).item())\n",
    "    G_loss_train.append(torch.mean(torch.FloatTensor(G_losses)).item())\n",
    "    m_loss_train.append(torch.mean(torch.FloatTensor(mse_losses)).item())\n",
    "\n",
    "    print('Training [%d/%d]: loss_d: %.5f, loss_g: %.5f, loss_mse: %.5f' % (\n",
    "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses)), torch.mean(torch.FloatTensor(mse_losses))))\n",
    "    \n",
    "    # validation\n",
    "    D_losses,G_losses, mse_losses = [],[],[]\n",
    "    for batch_idx,(((x_d, x_g), _)) in enumerate(test_loader_d_g):\n",
    "        x_d=x_d.to(device)\n",
    "        x_g=x_g.to(device)\n",
    "        G_losses.append(G_val(x_d, x_g)[0])\n",
    "        D_losses.append(D_val(x_d, x_g))\n",
    "        mse_losses.append(G_val(x_d, x_g)[1])\n",
    "    \n",
    "    D_loss_val.append(torch.mean(torch.FloatTensor(D_losses)).item())\n",
    "    G_loss_val.append(torch.mean(torch.FloatTensor(G_losses)).item())\n",
    "    m_loss_val.append(torch.mean(torch.FloatTensor(mse_losses)).item())\n",
    "        \n",
    "    print('Validation [%d/%d]: loss_d: %.5f, loss_g: %.5f, loss_mse: %.5f' % (\n",
    "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses)), torch.mean(torch.FloatTensor(mse_losses))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D_loss_train': [0.6959559321403503, 0.6937533617019653, 0.6933814883232117, 0.6930937767028809, 0.6930562257766724, 0.6931020021438599, 0.6931285858154297, 0.6931386590003967, 0.6931332349777222, 0.6930601000785828], 'D_loss_val': [0.6233959197998047, 0.6882113814353943, 0.6927973031997681, 0.6927571296691895, 0.6929111480712891, 0.6928166151046753, 0.6929084658622742, 0.6932144165039062, 0.6926721334457397, 0.6925209760665894], 'G_loss_train': [0.8923910856246948, 0.8539129495620728, 0.7456011772155762, 0.7316796779632568, 0.7245588302612305, 0.7210081219673157, 0.7198490500450134, 0.7178289294242859, 0.7165964245796204, 0.7161259651184082], 'G_loss_val': [0.9892879128456116, 0.7649821639060974, 0.7376859188079834, 0.7273378968238831, 0.7228130102157593, 0.7226331233978271, 0.7205546498298645, 0.7217980623245239, 0.715857207775116, 0.7165733575820923], 'm_loss_train': [0.10779856890439987, 0.09723976254463196, 0.05165832117199898, 0.0385112464427948, 0.03213122487068176, 0.028434135019779205, 0.025945361703634262, 0.02429729513823986, 0.022950544953346252, 0.022459449246525764], 'm_loss_val': [0.11143099516630173, 0.07881081104278564, 0.042818162590265274, 0.03394391015172005, 0.029653357341885567, 0.02635032869875431, 0.02492242492735386, 0.02491440623998642, 0.02146337553858757, 0.021334942430257797]}\n"
     ]
    }
   ],
   "source": [
    "loss_dict = {'D_loss_train': D_loss_train,\n",
    "'D_loss_val': D_loss_val,\n",
    "'G_loss_train': G_loss_train,\n",
    "'G_loss_val': G_loss_val,\n",
    "'m_loss_train': m_loss_train,\n",
    "'m_loss_val': m_loss_val}\n",
    "\n",
    "print(loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #===================Test the model===================#\n",
    "# index = 28\n",
    "\n",
    "# for batch_idx,(((x_d, x_g), _)) in enumerate(test_loader_d_g):\n",
    "#     if batch_idx == index:\n",
    "#         data = x_g[0][0]\n",
    "#         original = x_d[0][0]\n",
    "#         z = data.view(-1, mnist_dim)\n",
    "\n",
    "#         fig, ax = plt.subplots(1,3)\n",
    "#         ax[0].imshow(transforms.ToPILImage()(data), cmap='Greys')\n",
    "#         ax[1].imshow(G(z).detach().numpy().reshape(28,28), cmap='Greys')\n",
    "#         ax[2].imshow(transforms.ToPILImage()(original), cmap='Greys')\n",
    "#         plt.show()\n",
    "\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=torch.load('sample_pics/mask_seven.pth')\n",
    "original=torch.load('sample_pics/og_seven.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAADNCAYAAAAYNBLcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwHElEQVR4nO3deXRUVbo28CcJSZgykIQkRIgkEKbMAoEAAkIk2q2CgKBtQ1SuSEtwABzoK3DbZV8UFFBEuGozeG0UbRka9NLN3M1lFOEiUwQ6TIaEIVNlnvb3R76UqZz3hCpITg08v7WyFnlzhn1O7TrsOvXu97gppRSIiIiIDOJu7wYQERHRnYWDDyIiIjIUBx9ERERkKA4+iIiIyFAcfBAREZGhOPggIiIiQ3HwQURERIbi4IOIiIgMxcEHERERGapJBx+dO3fG0aNHm3KThvnrX/+Kl19+GQBw/vx5LF++3OLv1h7b+fPn4eHhgYSEBPPPuXPnzH/fvHkzevTogaioKIwePRqFhYXidmpqajBt2jR06dIFXbt2xYcffmj+24oVK9CrVy8kJibihx9+MMfnzJmDNWvW2HLYuubMmYM///nPt7RuQkICTCZTk7QDAPLz8/H222832faciV6/+7d/+zfs3LnT+AY1s6CgIJw/f/6my7m5uSE/P7/Z29PQrl27kJCQAODO7pfNpaKiAq+99hq6du2Knj17IjY2FqtXr250HWuvVfWv8beq/ut/q+2g/081obvvvlsdOXKkKTdpFzt37lTx8fEWMWuPLTMzU/n5+Yl/M5lMKjg4WJ06dUoppdTUqVPVzJkzxWVXr16thg0bpqqqqtSNGzdUeHi4On78uFJKqc6dO6uioiK1e/duNWbMGKWUUsePH1cjR4606vicTWPnVCmlKisrjWuMwRzhPWXk+Q0MDFSZmZk3XQ6AysvLu+lyVVVVt9+oeupfG27WL8l2TzzxhBo9erQqKipSStWe4x49eqhPP/1UXN7o9770fwPdmmb72mXo0KGYMWMGBg8ejPDwcMyePRvfffcdBg0ahM6dO2PhwoXmZWfOnIm+ffsiISEBgwcPRkZGhvlvGzduRM+ePREfH4/XXnvN4pPRmTNn8Otf/xp9+/ZFXFycxd2B+u666y5kZWUBAMaNG4cBAwYAAMrLyxEYGIjy8nKsWrUKo0aNAgBMmTIFGRkZSEhIwCOPPGLezrp165CcnIyIiAi89dZbNp+T//mf/0FiYiJ69OgBAHj++efxxRdfiMuuXbsWzz77LDw8PBAQEIDx48ebl/Xw8EBZWRmKi4vh5eWFmpoavPzyy3j//fcb3f9//Md/YNy4cXj44YfRrVs3PPTQQzh+/DhSU1PRrVs3PPHEE6ipqQEAPPXUU1i8eDEAYNOmTYiLi0NCQgJiYmKwceNGAMBbb72Fnj17mu/wXLhwAYDlp9LOnTtjzpw54nk7ffo0kpOTER0djdGjR2PEiBFYtWqVpt1TpkyByWRCQkIC+vTpA6C2f73wwgtITk7GiBEjUFVVhdTUVPTp0wfR0dH4zW9+g+LiYvM2Vq5ciYSEBMTHx6NPnz7mPvS3v/0NgwYNQu/evZGUlGS+m3DmzBkMHDgQ8fHxiI2NxRtvvNHouTXa0KFDsWHDBgC1r9Vzzz2H4cOHo1u3bhg9ejQqKioAAJWVlXj99deRlJSEhIQEjBs3Dnl5eQCANWvWoF+/fkhMTER8fDw2bdpksf3657ehp556CpMnT0ZKSgoiIiLwzDPP4ODBgxg6dCgiIyMxffp087Jnz55FSkqKuQ/VtRuo/TTas2dPxMXF4dVXX7XYh7Xv7/pWrVqF++67D2PGjEFsbCwOHjyIQ4cOYdiwYejTpw8SExPx9ddfAwCuXbuGESNGIDY2FnFxcXj66afN26i7FgC1dyuHDh2q2ZfUL+nWnTlzBhs2bMDHH3+MNm3aAKi9frz33nv4wx/+AKD2zkN0dDQmTZqEhIQErF+/3uJaZTKZMH78ePTo0QP33nsvnnvuOTz11FMALF/XXbt2ISYmBs8//zzi4+MRHR2N77//HgBuei3RU78dtlxrG3sfNnaNNJlMePbZZ5GUlIS4uDhMnjzZ/L53Ck05kqn/KW3IkCFqzJgxqqqqSuXm5ipfX181depUVVNToy5fvqzatGlj/uRy9epV8za++OILlZqaqpRSKicnRwUEBJjvFKxYsUIBUJmZmaqqqkr17t3b/Lfi4mIVGxurDh48qGnXhAkT1OrVq1V1dbXq1q2b6tWrlyooKFDbtm1T999/v1JKqZUrV5rvHOjd+Zg2bZpSSqlr164pX19fdfnyZc2+MjMzVYsWLVSfPn1UYmKi+sMf/mD+9PXuu++qyZMnm5ctLi5W7u7u4ug9JiZG7d271/z70qVL1YQJE5RSSn3zzTcqMTFRDR48WJ06dUotWbJELV68WHhFLM2dO1dFRESo3NxcVVNTowYPHqz69eunCgsLVWVlpYqPj1ebN29WSimVlpamFi1apJRSKi4uztyW6upqlZeXp3Jzc5Wfn58qKSkxH0tpaalSyvJTaWPnrU+fPmrFihVKKaVOnjypvL291cqVK8Vz2vAT5pAhQ1RqaqqqqKhQSilVU1Ojrl+/bv73lClT1Lx585RSta9n586dVVZWlrmtxcXF6ty5c6p///6qoKBAKaXUmTNnVGhoqCorK1MvvPCC+s///E/z/m7cuHHT89sc9O58DBkyRK1fv14pVftaJSUlqeLiYlVVVaUGDBig1qxZo5RS6o9//KN68803zeu9+eab6vnnn1dKKXX9+nVVU1OjlKo9xyEhIaqsrMy8/frnt6G0tDTVv39/VVpaqsrLy1WXLl3UqFGjVEVFhSoqKlLBwcHmO3VJSUlq+fLlSimlfvrpJxUQEKDOnz9vfn+fOHFCKaXUf/3Xf1n9/obOnY+VK1eqVq1aqdOnTyullMrLy1MJCQnm1/7atWuqU6dO6vLly2rhwoUW78e617j+tUAppTZt2qSGDBmilOKdj+a0du1aFRcXp4nn5uYqAOrq1atq586dys3NTe3atcv89/rXqpkzZ6q0tDRVU1OjCgsLVUxMjEpLS1NKaa/xHh4eav/+/UoppZYtW6ZGjBihlLr5tUTvzkf9dthyrW3sfdjYNfLZZ59Vq1evNrdz0qRJav78+Vafb3tr0ZwDm7Fjx8LDwwPt2rVDZGQkHnroIbi5ueGuu+5C+/btcf78eSQkJGDr1q1YsmQJTCYTampqkJubCwDYv38/4uLizHcK0tLSMGXKFABARkYGTpw4gccff9y8P5PJhJMnT6Jv374W7UhJScG2bdsQHR2N+Ph4hISEYNeuXdi3bx+GDx9u9fH85je/AVD7vXRkZCQyMzNx1113WSzToUMH/PzzzwgODkZubi7Gjx+P9957T/Op7naMHj0ao0ePBgBcunQJGzduxJYtW/D73/8e586dQ9euXfHHP/5RXHfEiBFo164dAOCee+6Bt7c3fHx8AACJiYk4c+aMZp3hw4fjxRdfxNixYzFixAgkJCSguroaUVFR+O1vf4sRI0bg17/+NTp27CjuUzpvPj4+OHr0KCZOnAgA6NmzJwYNGmTTefjtb38LT09PAIBSCosWLcK3336LqqoqFBQUmO9wffvtt5gwYQI6dOgAAGjdujUAYMuWLTh79iwGDx5s3qa7uzsuXryIwYMH45VXXkFRURGGDBmClJQUm9pmtEcffdR8XElJSeY8ow0bNqCgoADffPMNgNrv1Dt37gwAyMzMxJNPPonLly+jRYsWyM3NRWZmpvn9Vv/8SkaOHImWLVsCAGJjY5GamgpPT094enqiV69eOHPmDMLDw/HDDz/gf//3fwEAUVFRGDRoEP75z3/C19cXcXFx6NWrFwBg0qRJmDZtGgDb3t8NDRgwAN27dwcA7N27F//617/w4IMPWiyTkZGB/v37Y9GiReY7tA888MBNzjI5gsjISAwZMkT82/bt27Fo0SK4ubnBx8cH48ePx9mzZ8Vlu3btin79+gEAkpOT8e677wJo/FpiC2uvtXrvw7CwsEavkRs2bMC+ffvM3yKUlpbCw8PD5nbaS7MOPuouTEDtVwUNf6+qqsLFixeRnp6OQ4cOoUuXLjh27JjFfwZ6lFIICAiwKgk0JSUFs2bNQq9evZCSkoKQkBBs27YN+/btw7Jly275eKqqqjTLeHt7Izg4GAAQEBCAZ555BmvWrMGrr76K8PBwbN261bzs+fPn0aFDB7RooX0ZwsPDceHCBSQnJ5uXDQ8P1yz34osvYuHChdi1axeysrKwdu1apKWlYefOnbjvvvtuegzWHNPChQtx4sQJ7Ny5E2lpaXjyySfx6quvYv/+/di7dy927dqF/v3744svvsC99957S+cNqP26xhZt27Y1/3vNmjXYsWMHdu/eDV9fX3zwwQfYsWNHo+srpXD//feLSbpRUVEYMGAAtm7dig8//BCLFy/Gd999Z1P7jKR3jpVSWLJkifjVyeOPP463334bY8eOBVDbX8vKysx/r39+rdnn7b7O9eO2vL8bqt9upRSio6Oxd+9ecdmjR49i27ZtWLduHWbPno0jR46gRYsWqK6uNi9T/5xQ86n7D/nGjRsIDAw0x/ft24dOnTqhffv2AG7eL+tr7Jqi119v5Vpizfb19nez96He8Sil8M0336Bbt242t80R2H2qbUFBATw9PdGhQwcopSy+1+3fvz+OHTtmzgH5/PPPzd9pde/eHb6+vli5cqV5+bNnz5rvmtQXFhYGPz8/LF++HCkpKbjvvvuwefNmnD9/Hvfcc49meV9fXxQUFNzS8Vy9ehWVlZUAanNK1q1bh8TERADAAw88gB9++AGnT58GAHz00UcWn+zqe+yxx/DJJ5+guroaubm5WLt2LcaPH2+xzNq1a80Z4cXFxeaO6e7ujqKioltqv+T06dOIjo5Geno6fve732H//v0wmUzIycnBvffei9mzZ2PQoEE4cuSI1dv09fVFfHw8Pv/8cwC1n0T37Nmju2xpaWmj32fm5eUhKCgIvr6+MJlMFrkjDz/8MD7//HNcuXIFAFBSUoKSkhKkpqZi27ZtOHbsmHnZgwcPAqj9/jkkJAQTJ07E/PnzsX//fquPzZGMGjUKixYtQklJCYDaYz9x4gSA2nMWEREBoPa9VZcL0pR8fHxwzz33mN+nZ8+exZ49ezB48GAkJyfj2LFj5vfDihUrbun93ZgBAwYgMzMT27ZtM8eOHj2KiooKZGZmom3bthg3bhyWLFmCn376CUVFRejatSuOHTuG0tJSVFVV6c4gs6ZfkvWioqLw8MMPY/Lkyeb+ev78ecyYMQOzZ8+2ahvDhg3D6tWroZRCUVERvvrqK5vb0di1pDnovQ9vdo0cNWoU3nnnHfMgJi8vT/cujyNq1jsf1oiNjcXjjz+O6OhoBAYGWiR6BQcH49NPP8WoUaPg7e2N+++/H23btoW/vz9atGiBzZs346WXXsKiRYtQXV2NoKAg3QtFSkoKNm/ejMjISABAaGgoEhMT4e6uHX/FxcUhOjoaMTExiIyMxF//+lerj2fPnj2YM2eOeWQ7bNgw/Pu//zuA2gtx3fFUVVUhJibGYhpZQkICvvvuO4SFhWHChAk4dOgQoqKi4ObmhunTpyM2Nta8bF5eHpYuXYq///3vAGoHNp9++ini4uIQERHRpLeQf//73yMjIwNeXl5o3bo1li1bhoKCAowdO9Y86ImKikJaWppN2/3ss8/wzDPPYMGCBejatSv69u0Lf39/zXIBAQGYOHEi4uLi0LZtW3NiWH0TJ07Exo0b0b17d7Rv3x733nuvOQF28ODBmDt3LlJTU+Hm5gYvLy/85S9/QdeuXbFmzRo899xzKCkpQUVFBRITE7FmzRr85S9/weeff25O6G049dpIdV9n1LFlIPTaa6+hvLwc/fr1Mw9OX3vtNURHR+P999/H2LFj4e/vj2HDhol31prCn//8Z0yZMgUffvgh3Nzc8Omnn5r3tWLFCjz66KPw8vLCAw88YP7Ea+v7W0+7du3w7bffYubMmZgxYwYqKysRHh6ODRs2YNeuXVi4cKH5vbpgwQL4+fmhf//++NWvfoWYmBh06NABAwcOxIEDBzTbtqZfkm0+++wzvPHGG4iNjYWXlxc8PDzwyiuv4JlnnrFq/Tlz5mDSpEno2bMngoKCEB8fL15TGtPYtaQ5NPY+bOwauWjRIrz++utISEiAu7s7WrRogfnz56Nr167N1tam5KaUUvZuRGNMJpP5e7INGzZg1qxZOHXqlJ1bRU2hqKgIbdq0gZubGzIzM5GcnIxDhw6hU6dO9m4aETmhyspKVFdXo2XLliguLkZqaiqmTZumuWvsLFz5Gmn3Ox83s2TJEqxduxbV1dXw9fVlERcXsnfvXrzyyisAgOrqaixatMgl3lREZB95eXl48MEHUV1djbKyMowcORLjxo2zd7NumStfIx3+zgcRERG5FrsnnBIREdGdhYMPIiIiMhQHH0RERGSoZks4Xbp0KRYsWIDs7GzEx8djyZIlSEpKuul6NTU1yMrKgo+Pj81Fp4jqKKVgMpkQFhYmTqduDPsu2RP7Ljkrm/puc9Rs//LLL5WXl5dasWKFOnHihHr22WeVv7+/ysnJuem6ly5dUgD4w58m+bl06RL7Ln+c8od9lz/O+mNN322W2S79+vVD3759zdVKa2pq0KlTJ0ybNg2vv/56o+sWFBTA398fly5dgq+vb1M3je4QhYWF6NSpE/Lz8+Hn52f1ek3Rdz/55BPzc1aA2mcuNCSV1Adgro5bX93zIeqztoKttG8A4jNb6p6yWV9AQIAmVvfEYmvWly4vUqx+6ek65eXl4n4k0qcsLy8vTUw673r7kbZZv+x6nbonsNand1ktLCzUxBoWwSopKcHTTz9tl77L6y7dDluuu03+tUtFRQUOHz6MWbNmmWPu7u5ISUnBvn37NMuXl5dbvPlNJhOA2tKyfBPQ7bLlFnJT9d3WrVtbDD6kNtgy+JD+c5P+o7eF9B+ztM36x1FHr5y4tL61A5JWrVppYrZ85SA9UMvawYfew7isHXxI50hv8CG9vtL6gH36Lq+71BSs6btNnnB6/fp1VFdXIyQkxCIeEhKC7OxszfLz5s2Dn5+f+cdVCqiQ82HfJWfFvkvOxu6zXWbNmoWCggLzz6VLl+zdJCKrsO+Ss2LfJXtr8q9dgoKC4OHhgZycHIt4Tk4OQkNDNct7e3vD29u7qZtBZLOm6ruVlZUWt9elHAm970Ol3Acpv0Par/QVh97tT2u/dpH2La0LyF/HSF9pSPkmxcXFmpje1xHWfh0jLSfld+jlxVj7NZT0pF1bzlHD/eu1pzG87pKzafI7H15eXujduze2b99ujtXU1GD79u1ITk5u6t0RNRn2XXJW7LvkbJqlzsf06dORlpaGPn36ICkpCYsXL0ZxcTGefvrp5tgdUZNh3yVnxb5LzqRZBh/jx4/HtWvXMGfOHGRnZyMhIQFbtmzRJEMRORr2XXJW7LvkTJqtwml6ejrS09Oba/NEzYZ9l5wV+y45i2YbfBDdqRomnEo1LPRqS+gV8GooODhYEyspKbF6P1K9Cak2RVVVlSYm1boAtMWyADl5Ukq6lNpz7do1cT9S0qdUC0Xat7QfveOREmOlBF4pcVOvcFlgYKAmVlZWZlV7iFyJ3afaEhER0Z2Fgw8iIiIyFAcfREREZCgOPoiIiMhQHHwQERGRoTjbhaiJNXyqrTTjpOEMhzrSE0WlWSjSo9mlWRJ6T7+VZmhIJcGl2S4+Pj7iNqVjkkqc37hxw6rlpJkpestKM1Patm2riUll3KUYID8BVyp/Lx23Xml4aUZSw7brzVAiciW880FERESG4uCDiIiIDMXBBxERERmKgw8iIiIyFBNOiZpYaWmpWIa7Pr2/S0mjBQUFmpi1Jdv1Eh+tXV9K7tRLYpW2KSVzSiXGf/75Z6v3I21TamdeXp4mVlRUpIl1795d3M/169c1MVvKs0v0kmhtXYbI2fHOBxERERmKgw8iIiIyFAcfREREZCgOPoiIiMhQTDglamKVlZWoqKgw/y4lfeolgpaWlmpiUqVOqVKmn5+f1W2UqnJaW11VqiYKyAme/v7+mpiUQJuQkKCJSUmkgHyOzp8/r4lJVVyDgoI0Mb0Ez+DgYKuWlZKHpSquess2TPRlhVO6E/DOBxERERmKgw8iIiIyFAcfREREZCgOPoiIiMhQTDilRl29elUTkxLx6BceHh4Wj2OXEievXbsmrtumTRtNTHrUvVTlU9pmWFiYbhsbkpJLpf2cOXNG3KaUjOnj46OJSUmXUhKrXtLm2bNnNbGcnBxNrFevXppY/dflZvu5++67xXhD0usrHTcgJ/o2jLHC6Z3lxIkTmlj79u01MVe77vLOBxERERmKgw8iIiIyFAcfREREZCgOPoiIiMhQTDi9Q82YMUMTe++99zSxd955x6rl6BdeXl4WSaLSo+GlxFIAqKqq0sSkaqbZ2dmamFRNVEqwBKyvKColP+ptU0qMlRIsT58+rYlJSaw//vijuJ89e/ZY1ab6VWbrREZGamJ6SblSsq2UqCsly0r71qNX7ZZcj3Tdff/99zWxF198URNztesu73wQERGRoTj4ICIiIkNx8EFERESG4uCDiIiIDMXBBxERERmKs13uUH/60580MSmbOjk52YjmuJSysjKLEuLe3t6aZfRmQ0ilvs+dO6eJSTNgysvLNbHS0lJxP1FRUZpYUFCQJia1XW+mTsuWLTUxk8mkid11112aWG5uriZWUFAg7ufSpUuaWGBgoLhsQwEBAZqYNFsFkGcpSaRzJM38AeTS8g1n1dgyU4Yck17Jfum6Kz3W4E647vLOBxERERmKgw8iIiIyFAcfREREZCgOPoiIiMhQd3TCqZS41qlTJ01Mr8xzbGzsLe/7woULVi979913a2IHDx7UxJKSkqzeppQIKPnVr35l9Taplpubm0VioVRi3JaE08zMTE3sp59+0sSk/ej1MynBs6ioSBOTSoz7+fmJ25TWl5JlL168qIlJ5d6PHz8u7kdKopWS9l544QVNTEoEPXLkiLgfaZtSeXVbStBLCacN9yPt15VI110pubewsFBcPyMjQxPr1auXJqaXGH07cnJyNDGpP8+cOVNcX7ruPvnkk5rYnXDd5Z0PIiIiMhQHH0RERGQoDj6IiIjIUBx8EBERkaFsTjj9xz/+gQULFuDw4cO4cuUK1q9fj1GjRpn/rpTC3Llz8cknnyA/Px8DBw7EsmXLxIqKRnrsscc0sfXr12tiVVVVmpiUDAcA1dXVt9yeyMhIq5eV9tO/f39NzNqKjLZo3bp1k2/TXozquyaTyaIfBQcHa5bRq94pxaUkVKnCqZTMJiWmAkBWVpYmJiX4SVVTpUqmgJycKpHWv379ulUxQE7WlWJS0qdU4VTveKT3nXQ+pMRGvdf3VpMgXf266yxJtlI7IyIiNDHp/aXn7bff1sRc6bqrx+Y7H8XFxYiPj8fSpUvFv8+fPx8ffPABli9fjgMHDqBNmzZITU0VM/GJjMS+S86KfZdcjc13Ph588EE8+OCD4t+UUli8eDHeeOMNjBw5EgDw2WefISQkBBs2bMDjjz+uWae8vNzi04Te9Cqi28W+S86KfZdcTZPmfGRmZiI7OxspKSnmmJ+fH/r164d9+/aJ68ybNw9+fn7mH6nOBlFzY98lZ8W+S86oSQcf2dnZAICQkBCLeEhIiPlvDc2aNQsFBQXmH6kADVFzY98lZ8W+S87I7hVOvb29xUdSNzWpsmDPnj2tWnf48OFN3RykpqaKcX9/f6vWtyWhiZqHXt9tGL969apmGb1Hrvv4+Ghi4eHhmphU5VN6NLzeJ9qwsDBNTEpsvXbtmiam936Vlg0KCtLEunTpoom1b99eE5Oq+OqRKlx27NhRE5MqvkpVKwH5OKWEVSkJVS9h0NfXVxOzR16GM1x3O3ToIMYfeeQRq9aXKgOfOnVKXFa67g4YMEATGzt2rFXrSlWpATmJWi/h2dU16Z2P0NBQANo3c05OjvlvRI6IfZecFfsuOaMmHXxEREQgNDQU27dvN8cKCwtx4MABJCcnN+WuiJoU+y45K/ZdckY2f+1SVFSEs2fPmn/PzMzE0aNHERAQgPDwcLz00kt46623EBUVhYiICMyePRthYWEWc9KJ7IF9l5wV+y65GpsHH99//z3uu+8+8+/Tp08HAKSlpWHVqlV49dVXUVxcjMmTJyM/Px+DBg3Cli1b7tjvtchxsO+Ss2LfJVdj8+Bj6NChjVajc3Nzw5tvvok333zzthpG1NTYd8lZse+Sq7H7bBejfPXVV7e87t///vcmbEmt77777rbWZyKZ4/Lz87OY7SCV/pZmAegtK5XqbtWqlSYmzZqo/2m5Pqkk9MWLFzUxqWS7FAPk2S5JSUmamDQL5W9/+5smpneOpE/z0ow0aTlpZopUhh0A3N21KXHS4xekkul6jzqQzlHDMu7N8ZgEe7md666zkGbV5OfnG98QJ8MHyxEREZGhOPggIiIiQ3HwQURERIbi4IOIiIgMdccknJIlvdLFdPsqKioskhilJNK2bduK60pluaXy6lJ5fSkZMjo6WtxP165dNbHg4GBNTCq5Xr/eRH0xMTGamJTYKpWYlpI79cp/SwmrQ4cO1cRKSko0MaksvVTyHJATeCsrKzUx6XikJFRALqvfMHnYw8NDXJcc0//93/9pYtJ7EZCvu3fqdGje+SAiIiJDcfBBREREhuLgg4iIiAzFwQcREREZigmnd6hly5bZuwkuy93d3SKBUko+lBIXG4s3FBQUpIlJ+5EqegK1Tz21RmRkpCaml6ApJdlJSZuenp6aWMMqn4B+wundd9+tiUnVULdt26aJSRVGc3Nzxf1IyaE+Pj6amJQ8LCW7AvLr4e/vb/G7K1U4vRP885//tHpZ6borJZnfCXjng4iIiAzFwQcREREZioMPIiIiMhQHH0RERGQoJpzayTfffCPGQ0JCNLFBgwZpYnv27LFqOT1KKauWkxIGbanIJ7XzX//6lyY2ceJEq7fpbPQe2S6RkhelRE4pcTEwMFAT00tgPXTokFXbzMnJ0cSk5FBArh567tw5TSwvL08TO3LkiCaml7T5008/aWLz58/XxKS+27CaKCAnkQLye1F6faT3g17SqLR+w+quUrVXalx2drYmtm7dOk1Mek0bi1vjo48+snpZ6borXQ/DwsI0MZPJpIllZGRYvW9Hu+6ylxMREZGhOPggIiIiQ3HwQURERIbi4IOIiIgMxYRTO3nsscesXlZKXrv33ns1MWuTSAFg1KhRVu1Hqr5nSwXGd999VxP77//+b6vXd0YeHh43fSy6lJwJABUVFZqYlLAqJaRJlUd//vlncT+nTp3SxA4fPqyJScmdBQUF4jalxFipr0jnRlpXrz9LVUpPnjypiUmJpO3atdPEpIqrgJxYKyWD+vn5aWI3btwQtym97g33LyXKUuM2bdqkiaWnpzf5fqQ+KVXX1SNdd5vDI488ook52nWXdz6IiIjIUBx8EBERkaE4+CAiIiJDcfBBREREhuLgg4iIiAzF2S528sQTT4hxa2eS2JJhLQkICLBqOb12Wksqcezq5aPd3NwsXp/i4mLNMlIpc0Au/y2VVZZmtvj7+2tiV69eFfcjzTjp3r27JibNdrpw4YK4TalsujQ7QNpmfn6+JqZXGl5aPy4uThOzdraLXrn4oKAgTUw671I7pTYC8swlaaYP2WbChAmamFQyfe3ateL60nX3q6++smrftlyLpf6Xmppq1boPPfSQJta3b19x2cjISE3M0a67jtUaIiIicnkcfBAREZGhOPggIiIiQ3HwQURERIZyU7bU5DZAYWEh/Pz8UFBQICZ3uTpry/dKJZhbtmxp9X6k8s+BgYFWr+/o7NGP6vb58ccf6yYc1vH29hbjRUVFmpiUsCr1kx49eljZUiArK0sTk5JgpXMntRGQ29mmTRtNTGq7VB5bL/laSqbr0qWLJqZXBr6htm3binEp+VcqpS4lEUpl8gE5ubTh+SwtLcWUKVPs0nd53f2FXlJ4Q1Jist7jE6QE8Dv1uss7H0RERGQoDj6IiIjIUBx8EBERkaE4+CAiIiJDscKpg7G2Wp4tyaUSLy+v21qf9Hl5eVmcX+lc61UbLCws1MSk11pKOJaSHPX2Y+3rLyVYduvWTVxW6rtS23/88UdNTEru69+/v7if5ORkTSwnJ8eq9rRv314Tk6q9AvL5lJJgpQqnegmHUoXThvt3tEqUdwJr++7t4nX3F+zlREREZCgOPoiIiMhQHHwQERGRoTj4ICIiIkPZlHA6b948rFu3DqdPn0arVq0wYMAAvPPOOxaP4i4rK8OMGTPw5Zdfory8HKmpqfjoo4/ExxuT/UiVJ12ZkX23YcKpVNVSr3qiVBlVWlaqvmlthVFATpIsKSnRxK5du6aJ6VVnlfYlJW1KyX3WtgcA8vPzNTGp6qq0TSmBVqr2CsjtlJJBpeRfHx8fcZvS+WjYJk9PT4vfed11PsOHDxfjd9p1tzE23fnYvXs3pk6div3792Pr1q2orKzEiBEjLC56L7/8MjZt2oSvv/4au3fvRlZWFkaPHt3kDSeyBfsuOSv2XXJFNt352LJli8Xvq1atQnBwMA4fPozBgwejoKAAf/rTn7BmzRoMGzYMALBy5Ur07NkT+/fvF6fOlZeXW3yyk6YaEt0u9l1yVuy75IpuK+ej7sFNAQEBAIDDhw+jsrISKSkp5mV69OiB8PBw7Nu3T9zGvHnz4OfnZ/7p1KnT7TSJyCrsu+Ss2HfJFdzy4KOmpgYvvfQSBg4ciJiYGABAdnY2vLy84O/vb7FsSEgIsrOzxe3MmjULBQUF5p9Lly7dapOIrMK+S86KfZdcxS1XOJ06dSqOHz+OPXv23FYDvL29dRPYqPncyVUUm7vvFhUVWVTCtLZqLSBXwJSS1PLy8jQxqfqm3uPipaqeZ8+e1cSkKo+XL18WtykleEZFRWli586d08QyMzM1sejoaHE/dZ/862uYpAlA858xIFcevXHjhrgfKfnX2muVlGQMyG1v2D/0qqMCvO46iw4dOojxO/m629AtnYn09HRs3rwZO3fuRMeOHc3x0NBQVFRUaLLRc3JyEBoaelsNJWoK7LvkrNh3yZXYNPhQSiE9PR3r16/Hjh07EBERYfH33r17w9PTE9u3bzfHMjIycPHiRfF5DERGYd8lZ8W+S67Ipq9dpk6dijVr1mDjxo3w8fExf5/o5+eHVq1awc/PD5MmTcL06dMREBAAX19fTJs2DcnJyboPiSIyAvsuOSv2XXJFNg0+li1bBgAYOnSoRXzlypV46qmnAACLFi2Cu7s7xowZY1Hshsie2HfJWbHvkiuyafChlLrpMi1btsTSpUuxdOnSW24UUVNj3yVnxb5LruiWZ7uQfeXm5mpidfP+yb6qq6stZjtIJb2lmSGAXCZcmgEjzcSQ/pPSKx7l6+uriUkZ+lJp9+vXr4vbrF9Svo40W0aa7SLN6JHW1YtL56i6uloTk0qz681AkGYpSTNRpNdCr+3SvhrGOCPCcUnXXamfTZw40YjmODX2ciIiIjIUBx9ERERkKA4+iIiIyFAcfBAREZGhmHDqpDIyMjQxFhRyDP7+/hZJiFIiqF5SYatWrTSxsrIyTUxKcpOSQ6WS63ptksqMS2XYg4KCxG0GBwdrYleuXNHEfvzxR6v2o1edU2p7RUWFJiYlwErnUkq+1SM9gK2kpEQTM5lMVm+zYTv1SrOT/UnXXcnAgQObuSXOj3c+iIiIyFAcfBAREZGhOPggIiIiQ3HwQURERIZiwqmTOnz4sCbGhFPHkJOTY5E4KlU41Us4lRJJpUqbEqnSpl7CaXFxsSYmVVeVKqR27txZ3KZ0nFlZWZqYlJgqVTiVkkgBucKqt7e3JiYdu7XVYgE5+VeqkCodd15enrhNKbm1YdVUveMm+5Ouux9//LEmplfhln7BOx9ERERkKA4+iIiIyFAcfBAREZGhOPggIiIiQzHh1Emlp6fbuwmkw83NzSJJVEoilZI7ATnxUqroKVXVtCXhVEqmlCqX+vj4aGIBAQHiNqXHzZ88edKq/QQGBmpiUiInICeXSvu2NtFXen0AOTlVOm/SayGdNwCorKy86Tal15scA6+7TYd3PoiIiMhQHHwQERGRoTj4ICIiIkNx8EFERESG4uCDiIiIDMXZLkRNzMPDAx4eHo0uozejQYpLM1ak2RSenp6amFQOHAC8vLwabV8dacaH3uyQq1evamIhISGamDQrp23btppYaGiouB/pHOXk5Ghi0jmSZrtIs1UAucy5NFtFKn9fXl4ublPaf8PXV2+GEpEr4Z0PIiIiMhQHH0RERGQoDj6IiIjIUBx8EBERkaGYcErUxHx8fCySKqXkQ6nEOABcv35dE5MSSSVSkqtUDhyQkz6lsuVS2wsLC8VtWpsYKyW7Svu5du2auB8p4VTappTcaTKZNDG9c9SyZUtNLDc3VxOTjlvvNZOSUxuW2pdKxRO5Gt75ICIiIkNx8EFERESG4uCDiIiIDOVwOR913+fqfa9MZI26/mPk48nr9tXwO3spn0H67l9aFwCqqqo0MSm/Q1pOKoqlt75UPExqu1R8C5BzH6SYtL60H71CbdJrKhVDk/JApPOr10ektkvrS8vpnXepnWVlZeI+7NF3ed2l22HLddfhBh91CWGdOnWyc0vIFZhMJvj5+Rm2LwCYMmWKIfsj12aPvsvrLjUFa/qumzJyeG2FmpoaZGVlwcfHByaTCZ06dcKlS5fg6+tr76bdtsLCQh6PQZRSMJlMCAsLE2c9NAf2XefhyMfDvtu0HPm1vhWOfDy29F2Hu/Ph7u6Ojh07Avjl1rSvr6/DneTbweMxhlGfGuuw7zofRz0e9t2mx+MxhrV9lwmnREREZCgOPoiIiMhQDj348Pb2xty5c8XKi86Ix3PncLVzw+O5c7jaueHxOCaHSzglIiIi1+bQdz6IiIjI9XDwQURERIbi4IOIiIgMxcEHERERGYqDDyIiIjKUww4+li5dis6dO6Nly5bo168fDh48aO8mWe0f//gHHn74YYSFhcHNzQ0bNmyw+LtSCnPmzEGHDh3QqlUrpKSk4MyZM/Zp7E3MmzcPffv2hY+PD4KDgzFq1ChkZGRYLFNWVoapU6ciMDAQbdu2xZgxY5CTk2OnFjsGZ+2/7Lvsu+y7jsHV+69DDj7Wrl2L6dOnY+7cufjhhx8QHx+P1NRUXL161d5Ns0pxcTHi4+OxdOlS8e/z58/HBx98gOXLl+PAgQNo06YNUlNTNU+3dAS7d+/G1KlTsX//fmzduhWVlZUYMWIEiouLzcu8/PLL2LRpE77++mvs3r0bWVlZGD16tB1bbV/O3H/Zd9l32Xcdg8v3X+WAkpKS1NSpU82/V1dXq7CwMDVv3jw7turWAFDr1683/15TU6NCQ0PVggULzLH8/Hzl7e2tvvjiCzu00DZXr15VANTu3buVUrVt9/T0VF9//bV5mVOnTikAat++ffZqpl25Sv9l373zsO86Llfrvw5356OiogKHDx9GSkqKOebu7o6UlBTs27fPji1rGpmZmcjOzrY4Pj8/P/Tr188pjq+goAAAEBAQAAA4fPgwKisrLY6nR48eCA8Pd4rjaWqu3H/Zd10b+65jc7X+63CDj+vXr6O6uhohISEW8ZCQEGRnZ9upVU2n7hic8fhqamrw0ksvYeDAgYiJiQFQezxeXl7w9/e3WNYZjqc5uHL/Zd91bey7jssV+28LezeAnMfUqVNx/Phx7Nmzx95NIbIJ+y45M1fsvw535yMoKAgeHh6ajN2cnByEhobaqVVNp+4YnO340tPTsXnzZuzcuRMdO3Y0x0NDQ1FRUYH8/HyL5R39eJqLK/df9l3Xxr7rmFy1/zrc4MPLywu9e/fG9u3bzbGamhps374dycnJdmxZ04iIiEBoaKjF8RUWFuLAgQMOeXxKKaSnp2P9+vXYsWMHIiIiLP7eu3dveHp6WhxPRkYGLl686JDH09xcuf+y77o29l3H4vL9184Jr6Ivv/xSeXt7q1WrVqmTJ0+qyZMnK39/f5WdnW3vplnFZDKpI0eOqCNHjigAauHCherIkSPqwoULSiml3n77beXv7682btyojh07pkaOHKkiIiJUaWmpnVuu9bvf/U75+fmpXbt2qStXrph/SkpKzMtMmTJFhYeHqx07dqjvv/9eJScnq+TkZDu22r6cuf+y77Lvsu86Blfvvw45+FBKqSVLlqjw8HDl5eWlkpKS1P79++3dJKvt3LlTAdD8pKWlKaVqp33Nnj1bhYSEKG9vbzV8+HCVkZFh30brkI4DgFq5cqV5mdLSUvX888+rdu3aqdatW6tHH31UXblyxX6NdgDO2n/Zd9l32Xcdg6v3XzellGreeytEREREv3C4nA8iIiJybRx8EBERkaE4+CAiIiJDcfBBREREhuLgg4iIiAzFwQcREREZioMPIiIiMhQHH0RERGQoDj6IiIjIUBx8EBERkaE4+CAiIiJD/T8h7+bWpaZVlgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "z = data.view(-1, mnist_dim)\n",
    "\n",
    "fig, ax = plt.subplots(1,3)\n",
    "ax[0].imshow(transforms.ToPILImage()(data), cmap='Greys')\n",
    "ax[0].set_title(f'Image with {masking_rate*100}% missing traces',size=8)\n",
    "ax[1].imshow(G(z).detach().numpy().reshape(28,28), cmap='Greys')\n",
    "ax[1].set_title(f'Linear model result',size=8)\n",
    "ax[2].imshow(transforms.ToPILImage()(original), cmap='Greys')\n",
    "ax[2].set_title(f'Original image',size=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #===================Visualise the feature map===================#\n",
    "# ## ref: https://ravivaishnav20.medium.com/visualizing-feature-maps-using-pytorch-12a48cd1e573\n",
    "# ## ref: https://www.analyticsvidhya.com/blog/2020/11/tutorial-how-to-visualize-feature-maps-directly-from-cnn-layers/\n",
    "# import random\n",
    "# index = 7\n",
    "# model_children = list(G.children())\n",
    "# print(model_children)\n",
    "\n",
    "# for batch_idx,(((x_d, x_g), _)) in enumerate(validation_loader_d_g):\n",
    "#     x_d=x_d.to(device)\n",
    "#     x_g=x_g.to(device)\n",
    "\n",
    "#     if batch_idx == index:\n",
    "#         data = x_g[0][0]\n",
    "#         original = x_d[0][0]\n",
    "#         z = data.view(-1, mnist_dim)\n",
    "\n",
    "#         fig, ax = plt.subplots(1,3)\n",
    "#         ax[0].imshow(transforms.ToPILImage()(data), cmap='Greys')\n",
    "#         ax[0].set_title(f'Image with {masking_rate*100}% missing traces',size=8)\n",
    "#         ax[1].imshow(G(z).cpu().detach().numpy().reshape(28,28), cmap='Greys')\n",
    "#         ax[1].set_title(f'Reconstructed image',size=8)\n",
    "#         ax[2].imshow(transforms.ToPILImage()(original), cmap='Greys')\n",
    "#         ax[2].set_title(f'Original image',size=8)\n",
    "\n",
    "#         fig = plt.figure(figsize=(30, 35))\n",
    "#         image = x_g\n",
    "#         tensor_dict = {}\n",
    "#         layer = 0\n",
    "        \n",
    "#         for count,layer in enumerate(model_children[:]):\n",
    "#             name = str(layer)\n",
    "#             name = name.split(\"(\")[0]\n",
    "\n",
    "#             image = layer(image)\n",
    "\n",
    "#             # save the output tensor of each layer\n",
    "#             tensor_dict[count] = image\n",
    "\n",
    "#             # plotting params\n",
    "#             channel = image.shape[1]\n",
    "#             size = image.shape[2]\n",
    "#             elem_per_row = 12\n",
    "#             target_width = elem_per_row * 28\n",
    "\n",
    "#             a = fig.add_subplot(4, 1, count+1)\n",
    "\n",
    "#             images = []\n",
    "#             used = set()\n",
    "#             for i in range(min(target_width // size , channel)):\n",
    "#                 new_ind = random.randint(0, channel - 1)\n",
    "#                 while new_ind in used:\n",
    "#                     new_ind = random.randint(0, channel - 1)\n",
    "#                 used.add(new_ind)\n",
    "#                 images.append(image[0][new_ind].reshape(size,size))\n",
    "\n",
    "#             # create image belt\n",
    "#             combined_image = torch.hstack(images)\n",
    "\n",
    "#             imgplot = plt.imshow(combined_image.cpu().detach().numpy(), cmap='Greys')\n",
    "#             imgplot = plt.axis('off')\n",
    "#             a.set_title(f'{name} {image.shape}', fontsize=20)\n",
    "\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
